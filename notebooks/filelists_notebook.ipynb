{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.use_jedi = False\n",
    "%pdb off\n",
    "# %load_ext viztracer\n",
    "# from viztracer import VizTracer\n",
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Any, List, Dict, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import globus_sdk\n",
    "from globus_sdk import AccessTokenAuthorizer, TransferClient, TransferData\n",
    "from globus_sdk.scopes import TransferScopes\n",
    "\n",
    "from phoglobushelpers.PhoGlobusHelper import GlobusConnector, KnownEndpoints\n",
    "from phoglobushelpers.compatibility_objects.Bookmarks import Bookmark, BookmarkList\n",
    "from phoglobushelpers.compatibility_objects.Files import File, FilesystemDataType, FileList\n",
    "from phoglobushelpers.compatibility_objects.Tasks import FatalError, Task, TaskList\n",
    "\n",
    "from phoglobushelpers.path_helpers import convert_filelist_to_new_parent, read_lines_from_file, find_matching_parent_path, known_global_data_root_parent_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please go to this URL and login: https://auth.globus.org/v2/oauth2/authorize?client_id=769d24e1-d1cc-4198-9ff7-2626485da449&redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code&scope=openid+profile+email+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall&state=_default&response_type=code&code_challenge=_MjchyvY0aOFEQwWj9QHduTYmJvlgo_dV2jLhsQbrhU&code_challenge_method=S256&access_type=offline\n",
      "\t Copied url to clipboard!\n",
      "My Endpoints:\n",
      "[c3a90494-2555-11ec-a47d-a50ad076c282] aleinbook-linux\n",
      "[84991054-07b4-11ed-8d83-a54cf61939f8] Apogee\n",
      "[6d0251c4-2585-11ec-9e35-3df4ed83d858] Cubix Win10\n",
      "[c3578f36-42a8-11ec-a50f-b537d6c07c1d] FlatEdge_Server\n",
      "[af3fcfce-f664-11ed-9a7d-83ef71fbf0ae] LNX00052_Fedora\n",
      "[20c84240-1eb1-11eb-81b7-0e2f230cc907] Pho Personal Laptop\n",
      "[560d3a12-be20-11ed-9916-cb2cff506ca5] Pho_Personal_Testix\n",
      "[debb635c-2556-11ec-a47d-a50ad076c282] rMBP Pink Dot\n",
      "[e414f584-2556-11ec-a0a7-6b21ca6daf73] rMBP Pink Dot\n",
      "[41b8fb12-516b-11ee-8a4b-b1038a0991ac] rMBP-PinkDot\n"
     ]
    }
   ],
   "source": [
    "connect_man = GlobusConnector.login_and_get_transfer_client()\n",
    "transfer_client = connect_man.transfer_client\n",
    "connect_man.list_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/home/halechr/repos/PhoGlobusHelpers/filelists/GreatGDriveMigration2023/dirsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023-09-28 - Newest Filelist Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\fileList_GreatLakes_pkls_2023-09-27-GL.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m output_filelist_transfer_dict\n\u001b[0;32m     11\u001b[0m \u001b[39m# read the lines from the files\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m output_file_lines_transfer_list \u001b[39m=\u001b[39m [(read_lines_from_file(filelist_source_path), read_lines_from_file(filelist_dest_path)) \u001b[39mfor\u001b[39;49;00m filelist_source_path, filelist_dest_path \u001b[39min\u001b[39;49;00m output_filelist_transfer_dict\u001b[39m.\u001b[39;49mitems()]\n\u001b[0;32m     13\u001b[0m output_file_lines_transfer_list\n",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m output_filelist_transfer_dict\n\u001b[0;32m     11\u001b[0m \u001b[39m# read the lines from the files\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m output_file_lines_transfer_list \u001b[39m=\u001b[39m [(read_lines_from_file(filelist_source_path), read_lines_from_file(filelist_dest_path)) \u001b[39mfor\u001b[39;00m filelist_source_path, filelist_dest_path \u001b[39min\u001b[39;00m output_filelist_transfer_dict\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m     13\u001b[0m output_file_lines_transfer_list\n",
      "File \u001b[1;32m~\\repos\\PhoGlobusHelpers\\src\\phoglobushelpers\\path_helpers.py:202\u001b[0m, in \u001b[0;36mread_lines_from_file\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_lines_from_file\u001b[39m(filename):\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    203\u001b[0m         lines \u001b[39m=\u001b[39m [line\u001b[39m.\u001b[39mstrip() \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file\u001b[39m.\u001b[39mreadlines()]\n\u001b[0;32m    204\u001b[0m     \u001b[39mreturn\u001b[39;00m lines\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\\\nfs\\\\turbo\\\\umms-kdiba\\\\Data\\\\fileList_GreatLakes_pkls_2023-09-27-GL.txt'"
     ]
    }
   ],
   "source": [
    "active_root_filelist_parent_path = Path('/nfs/turbo/umms-kdiba/Data/').resolve()\n",
    "\n",
    "output_filelist_transfer_dict = {\n",
    "\tPath('/nfs/turbo/umms-kdiba/Data/fileList_GreatLakes_pkls_2023-09-27-GL.txt'): Path('/nfs/turbo/umms-kdiba/Data/dest_fileList_LabWorkstation_pkls_2023-09-27-GL.txt'),\n",
    "\tPath('/nfs/turbo/umms-kdiba/Data/fileList_GreatLakes_global_pkls_2023-09-27-GL.txt'): Path('/nfs/turbo/umms-kdiba/Data/dest_fileList_LabWorkstation_global_pkls_2023-09-27-GL.txt'),\n",
    "\t# Path('/nfs/turbo/umms-kdiba/Data/fileList_GreatLakes_HDF5_2023-09-27-GL.txt'): Path('/nfs/turbo/umms-kdiba/Data/dest_fileList_LabWorkstation_HDF5_2023-09-27-GL.txt')\n",
    "}\n",
    "\n",
    "output_filelist_transfer_dict\n",
    "\n",
    "# read the lines from the files\n",
    "output_file_lines_transfer_list = [(read_lines_from_file(filelist_source_path), read_lines_from_file(filelist_dest_path)) for filelist_source_path, filelist_dest_path in output_filelist_transfer_dict.items()]\n",
    "output_file_lines_transfer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_data_folder_bookmark = Bookmark(bookmark_id='2a9f2464-01ad-11ee-be0e-195c41bc0be4', name='Apogee Data W', endpoint_id='84991054-07b4-11ed-8d83-a54cf61939f8', path='/~/W/Data/')\n",
    "lab_Turbo_data_folder_bookmark = Bookmark(bookmark_id='728fb3e8-2597-11ee-80c2-a3018385fcef', name='KDIBA Lab Turbo - Data Global Root Folder', endpoint_id='8c185a84-5c61-4bbc-b12b-11430e20010f', path='/umms-kdiba/Data/')\n",
    "lab_DataDen_data_folder_bookmark = Bookmark(bookmark_id='431c951e-96c6-11ed-9b93-19370d280681', name='kdiba DataDen Data', endpoint_id='ab65757f-00f5-4e5b-aa21-133187732a01', path='/umms-dibalab/Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_transfer_tasks = []\n",
    "## Perform the transfer:\n",
    "for filelist_source_path, filelist_dest_path in output_filelist_transfer_dict.items():\n",
    "\t# filelist_source_path = Path('../filelists/fileList_Greatlakes_2023-08-07.txt').resolve()\n",
    "\tsrc_lines = read_lines_from_file(filelist_source_path)\n",
    "\t# filelist_dest_path = '../filelists/dest_fileList_Apogee_2023-08-07.txt'\n",
    "\tdest_lines = read_lines_from_file(filelist_dest_path)\n",
    "\t\n",
    "\tendpoint_relative_src_lines = [a_line for a_line in src_lines]\n",
    "\tendpoint_relative_dest_lines = [a_line for a_line in dest_lines]\n",
    "\t_transfer_task = connect_man.batch_transfer_files(source_endpoint=lab_Turbo_data_folder_bookmark.endpoint_id, destination_endpoint=lab_Turbo_data_folder_bookmark.endpoint_id, filelist_source=endpoint_relative_src_lines, filelist_dest=endpoint_relative_dest_lines, max_single_file_wait_time_seconds=60.0)\n",
    "\tactive_transfer_tasks.append(_transfer_task)\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apogee_data_folder_bookmark = Bookmark(bookmark_id='2a9f2464-01ad-11ee-be0e-195c41bc0be4', name='Apogee Data W', endpoint_id='84991054-07b4-11ed-8d83-a54cf61939f8', path='/~/W/Data/')\n",
    "lab_Turbo_data_folder_bookmark = Bookmark(bookmark_id='728fb3e8-2597-11ee-80c2-a3018385fcef', name='KDIBA Lab Turbo - Data Global Root Folder', endpoint_id='8c185a84-5c61-4bbc-b12b-11430e20010f', path='/umms-kdiba/Data/')\n",
    "\n",
    "# endpoint_relative_src_lines = [a_line.replace('/umms-kdiba/Data/', '/', 1) for a_line in src_lines]\n",
    "# endpoint_relative_dest_lines = [a_line.replace('/~/W/Data/', '/', 1) for a_line in dest_lines]\n",
    "\n",
    "endpoint_relative_src_lines = [a_line for a_line in src_lines]\n",
    "endpoint_relative_dest_lines = [a_line for a_line in dest_lines]\n",
    "_transfer_task = connect_man.batch_transfer_files(source_endpoint=lab_Turbo_data_folder_bookmark.endpoint_id, destination_endpoint=apogee_data_folder_bookmark.endpoint_id, filelist_source=endpoint_relative_src_lines, filelist_dest=endpoint_relative_dest_lines, max_single_file_wait_time_seconds=60.0)\n",
    "_transfer_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_filelist_files = active_root_filelist_parent_path.glob('*.csv')\n",
    "filelist_dfs_list = []\n",
    "filelists_list = []\n",
    "for a_file in active_filelist_files:\n",
    "    print(f'a_file: {a_file}')\n",
    "    active_filelist_path = Path(a_file).resolve()\n",
    "    try:\n",
    "        user, filename_suffix = active_filelist_path.name.split('_')\n",
    "        print(f'\\t{user}')\n",
    "        parent_user_folder = Path(f\"Data/{user}\")\n",
    "        print(f'\\t{parent_user_folder}')\n",
    "        filelist_df = pd.read_csv(active_filelist_path, header=0, names=[\"name\", \"modified_dt\", \"size_bytes\"])\n",
    "        all_files_list = [parent_user_folder.joinpath(_a_file) for _a_file in filelist_df[\"name\"]]\n",
    "        filelist_df['name'] = all_files_list\n",
    "        filelist_dfs_list.append(filelist_df)\n",
    "        filelists_list.append(all_files_list)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f'encountered parser error for {active_filelist_path}')\n",
    "    except BaseException:\n",
    "        raise\n",
    "\n",
    "combined_filelist_df = pd.concat(filelist_dfs_list)\n",
    "combined_filelist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023-09-12 - New Filelist Transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Rachel_dirsize.csv\n",
      "\tRachel\n",
      "\tData/Rachel\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Nat_dirsize.csv\n",
      "\tNat\n",
      "\tData/Nat\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Utku_dirsize.csv\n",
      "\tUtku\n",
      "\tData/Utku\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Laurel_dirsize.csv\n",
      "\tLaurel\n",
      "\tData/Laurel\n",
      "encountered parser error for /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Laurel_dirsize.csv\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Kourosh_dirsize.csv\n",
      "\tKourosh\n",
      "\tData/Kourosh\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Bapun_dirsize.csv\n",
      "\tBapun\n",
      "\tData/Bapun\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Jahngir_dirsize.csv\n",
      "\tJahngir\n",
      "\tData/Jahngir\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Hiro_dirsize.csv\n",
      "\tHiro\n",
      "\tData/Hiro\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/KDIBA_dirsize.csv\n",
      "\tKDIBA\n",
      "\tData/KDIBA\n",
      "a_file: /home/halechr/Desktop/GreatGDriveMigration2023/dirsize/Output_dirsize.csv\n",
      "\tOutput\n",
      "\tData/Output\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>modified_dt</th>\n",
       "      <th>size_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.23.05 AM.csv</td>\n",
       "      <td>2022-08-11 12:41:13</td>\n",
       "      <td>1888355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.13.41 AM.csv</td>\n",
       "      <td>2022-08-11 12:40:48</td>\n",
       "      <td>731099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.12.49 AM.csv</td>\n",
       "      <td>2022-08-11 12:39:44</td>\n",
       "      <td>13590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/Rachel/Take 2021-08-27 12.59.15 PM.tak</td>\n",
       "      <td>2022-02-25 13:09:10</td>\n",
       "      <td>10307644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.34.03 AM.tak</td>\n",
       "      <td>2022-02-25 13:00:58</td>\n",
       "      <td>262879235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:10</td>\n",
       "      <td>537957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:08</td>\n",
       "      <td>748944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:03</td>\n",
       "      <td>735368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:08:58</td>\n",
       "      <td>500886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:08:55</td>\n",
       "      <td>472432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120589 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name          modified_dt  \\\n",
       "0          Data/Rachel/Take 2021-11-24 11.23.05 AM.csv  2022-08-11 12:41:13   \n",
       "1          Data/Rachel/Take 2021-11-24 11.13.41 AM.csv  2022-08-11 12:40:48   \n",
       "2          Data/Rachel/Take 2021-11-24 11.12.49 AM.csv  2022-08-11 12:39:44   \n",
       "3          Data/Rachel/Take 2021-08-27 12.59.15 PM.tak  2022-02-25 13:09:10   \n",
       "4          Data/Rachel/Take 2021-11-24 11.34.03 AM.tak  2022-02-25 13:00:58   \n",
       "..                                                 ...                  ...   \n",
       "690  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:10   \n",
       "691  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:08   \n",
       "692  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:03   \n",
       "693  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:08:58   \n",
       "694  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:08:55   \n",
       "\n",
       "     size_bytes  \n",
       "0       1888355  \n",
       "1        731099  \n",
       "2         13590  \n",
       "3      10307644  \n",
       "4     262879235  \n",
       "..          ...  \n",
       "690      537957  \n",
       "691      748944  \n",
       "692      735368  \n",
       "693      500886  \n",
       "694      472432  \n",
       "\n",
       "[120589 rows x 3 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def chunk_dataframe(df, size_limit_bytes):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into chunks based on a cumulative size in 'size_bytes' column.\n",
    "\n",
    "    :param df: DataFrame to split\n",
    "    :param size_limit_bytes: Max size for each chunk in bytes\n",
    "    :return: List of dataframes\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if current_chunk_size + row['size_bytes'] > size_limit_bytes:\n",
    "            chunks.append(pd.DataFrame(current_chunk))\n",
    "            current_chunk = []\n",
    "            current_chunk_size = 0\n",
    "\n",
    "        current_chunk.append(row)\n",
    "        current_chunk_size += row['size_bytes']\n",
    "\n",
    "    # Append any remaining data\n",
    "    if current_chunk:\n",
    "        chunks.append(pd.DataFrame(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def convert_filelist_to_new_parent(filelist_source: List[Path], original_parent_path: Path = Path(r'/media/MAX/cloud/turbo/Data'), dest_parent_path: Path = Path(r'/media/MAX/Data')):\n",
    "    \"\"\" Converts a list of file paths from their current parent, specified by `original_parent_path`, to their new parent `dest_parent_path` \"\"\"\n",
    "    filelist_dest = []\n",
    "    for path in filelist_source:\n",
    "        relative_path = str(path.relative_to(original_parent_path))\n",
    "        new_path = Path(dest_parent_path) / relative_path\n",
    "        filelist_dest.append(new_path)\n",
    "    return filelist_dest\n",
    "\n",
    "# Load filelist from disk\n",
    "# active_filelist_path = Path('/home/halechr/repo/PhoGlobusHelpers/filelists/session_results_filelist_2023-07-12.csv').resolve()\n",
    "# active_filelist_path = Path('/home/halechr/repos/PhoGlobusHelpers/filelists/GreatGDriveMigration2023/dirsize/Bapun_dirsize.csv').resolve()\n",
    "active_root_filelist_parent_path = Path('/home/halechr/Desktop/GreatGDriveMigration2023/dirsize').resolve()\n",
    "active_filelist_files = active_root_filelist_parent_path.glob('*.csv')\n",
    "filelist_dfs_list = []\n",
    "filelists_list = []\n",
    "for a_file in active_filelist_files:\n",
    "    print(f'a_file: {a_file}')\n",
    "    active_filelist_path = Path(a_file).resolve()\n",
    "    try:\n",
    "        user, filename_suffix = active_filelist_path.name.split('_')\n",
    "        print(f'\\t{user}')\n",
    "        parent_user_folder = Path(f\"Data/{user}\")\n",
    "        print(f'\\t{parent_user_folder}')\n",
    "        filelist_df = pd.read_csv(active_filelist_path, header=0, names=[\"name\", \"modified_dt\", \"size_bytes\"])\n",
    "        all_files_list = [parent_user_folder.joinpath(_a_file) for _a_file in filelist_df[\"name\"]]\n",
    "        filelist_df['name'] = all_files_list\n",
    "        filelist_dfs_list.append(filelist_df)\n",
    "        filelists_list.append(all_files_list)\n",
    "    except pd.errors.ParserError:\n",
    "        print(f'encountered parser error for {active_filelist_path}')\n",
    "    except BaseException:\n",
    "        raise\n",
    "\n",
    "combined_filelist_df = pd.concat(filelist_dfs_list)\n",
    "combined_filelist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65467.555576481"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_all_files_num_GB = combined_filelist_df.size_bytes.sum()/1e9\n",
    "total_all_files_num_GB # 65467.555576481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_limit = 750e9  # 750GB in bytes\n",
    "size_limit = 3999e9  # 4TB in bytes\n",
    "chunks = chunk_dataframe(combined_filelist_df, size_limit)\n",
    "num_chunks = len(chunks)\n",
    "print(f'num_chunks: {num_chunks}')\n",
    "# for chunk in chunks:\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>modified_dt</th>\n",
       "      <th>size_bytes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.23.05 AM.csv</td>\n",
       "      <td>2022-08-11 12:41:13</td>\n",
       "      <td>1888355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.13.41 AM.csv</td>\n",
       "      <td>2022-08-11 12:40:48</td>\n",
       "      <td>731099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.12.49 AM.csv</td>\n",
       "      <td>2022-08-11 12:39:44</td>\n",
       "      <td>13590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data/Rachel/Take 2021-08-27 12.59.15 PM.tak</td>\n",
       "      <td>2022-02-25 13:09:10</td>\n",
       "      <td>10307644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data/Rachel/Take 2021-11-24 11.34.03 AM.tak</td>\n",
       "      <td>2022-02-25 13:00:58</td>\n",
       "      <td>262879235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:10</td>\n",
       "      <td>537957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:08</td>\n",
       "      <td>748944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:09:03</td>\n",
       "      <td>735368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>693</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:08:58</td>\n",
       "      <td>500886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...</td>\n",
       "      <td>2023-06-01 02:08:55</td>\n",
       "      <td>472432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120589 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name          modified_dt  \\\n",
       "0          Data/Rachel/Take 2021-11-24 11.23.05 AM.csv  2022-08-11 12:41:13   \n",
       "1          Data/Rachel/Take 2021-11-24 11.13.41 AM.csv  2022-08-11 12:40:48   \n",
       "2          Data/Rachel/Take 2021-11-24 11.12.49 AM.csv  2022-08-11 12:39:44   \n",
       "3          Data/Rachel/Take 2021-08-27 12.59.15 PM.tak  2022-02-25 13:09:10   \n",
       "4          Data/Rachel/Take 2021-11-24 11.34.03 AM.tak  2022-02-25 13:00:58   \n",
       "..                                                 ...                  ...   \n",
       "690  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:10   \n",
       "691  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:08   \n",
       "692  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:09:03   \n",
       "693  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:08:58   \n",
       "694  Data/Output/2023-06-01/kdiba/vvp01/two/2006-4-...  2023-06-01 02:08:55   \n",
       "\n",
       "     size_bytes  \n",
       "0       1888355  \n",
       "1        731099  \n",
       "2         13590  \n",
       "3      10307644  \n",
       "4     262879235  \n",
       "..          ...  \n",
       "690      537957  \n",
       "691      748944  \n",
       "692      735368  \n",
       "693      500886  \n",
       "694      472432  \n",
       "\n",
       "[120589 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filelist_df = pd.read_csv(active_filelist_path, header=0, names=[\"name\", \"modified_dt\", \"size_bytes\"])\n",
    "filelist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21955757.33845806"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(filelist_df['size_bytes']/(1024.0 * 1024.0)) # Convert to GigaBytes (GB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           object\n",
       "modified_dt    object\n",
       "size_bytes      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filelist_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist_source = [Path(a_path_str).resolve() for a_path_str in filelist_df.Path]\n",
    "\n",
    "\n",
    "\n",
    "source_parent_path = Path(r'/media/MAX/cloud/turbo/Data')\n",
    "dest_parent_path = Path(r'/media/MAX/Data')\n",
    "# # Build the destination filelist from the source_filelist and the two paths:\n",
    "filelist_dest = convert_filelist_to_new_parent(filelist_source, original_parent_path=source_parent_path, dest_parent_path=dest_parent_path)\n",
    "filelist_dest\n",
    "\n",
    "# filelist_source\n",
    "# filelist_dest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
